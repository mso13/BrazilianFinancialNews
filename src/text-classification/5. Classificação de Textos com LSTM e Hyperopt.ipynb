{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variação de Hiperparâmetros (Biblioteca Hyperas/Hyperopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import LSTM, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "import _locale \n",
    "_locale._getdefaultlocale = (lambda *args: ['en_US', 'utf8'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \n",
    "    # 1. Load the dataset\n",
    "    json_data_path = '../crawlers/suno/data/results-full-suno-2020.json'\n",
    "\n",
    "    with open(json_data_path, 'r', encoding='utf8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    df_suno = pd.DataFrame(data)\n",
    "    \n",
    "    # Convert date to datetime format\n",
    "    df_suno['date'] = pd.to_datetime(df_suno['date'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "    # Set date column as index\n",
    "    df_suno.set_index('date', inplace=True)\n",
    "\n",
    "    # Order by date\n",
    "    df_suno.sort_index(inplace=True)\n",
    "    \n",
    "    # Convert text columns to string\n",
    "    df_suno['title'] = df_suno['title'].astype('string')\n",
    "    df_suno['full_text'] = df_suno['full_text'].astype('string')\n",
    "    \n",
    "    # Extract main columns\n",
    "    df_suno = df_suno[['title', 'full_text', 'topic']]\n",
    "    \n",
    "    # Select main topics\n",
    "    df_suno = df_suno[df_suno['topic'].isin(['Mercado', 'Economia', 'Internacional', 'Negócios'])]\n",
    "    \n",
    "    # 2. Preprocessing\n",
    "    def remove_emojis(sentence):\n",
    "\n",
    "        \"Remoção de Emojis nas mensagens de texto.\"\n",
    "\n",
    "        # Padrões dos Emojis\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                    u\"\\U00002702-\\U000027B0\"\n",
    "                    u\"\\U000024C2-\\U0001F251\"\n",
    "                    u\"\\U0001f926-\\U0001f937\"\n",
    "                    u'\\U00010000-\\U0010ffff'\n",
    "                    u\"\\u200d\"\n",
    "                    u\"\\u2640-\\u2642\"\n",
    "                    u\"\\u2600-\\u2B55\"\n",
    "                    u\"\\u23cf\"\n",
    "                    u\"\\u23e9\"\n",
    "                    u\"\\u231a\"\n",
    "                    u\"\\u3030\"\n",
    "                    u\"\\ufe0f\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        return emoji_pattern.sub(r'', sentence)\n",
    "\n",
    "    def remove_valores(sentence):\n",
    "        new_sentece = ''\n",
    "\n",
    "        for token in sentence.split():\n",
    "            if token.isdigit():\n",
    "                token = '<NUM>'\n",
    "            new_sentece += ' {}'.format(token)\n",
    "\n",
    "        return new_sentece\n",
    "\n",
    "    # Substituir símbolos importantes\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.replace('-feira', ''))\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.replace('+', 'mais '))\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.replace('-', 'menos '))\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.replace('%', ' por cento'))\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.replace('R$', ''))\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.replace('U$', ''))\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.replace('US$', ''))\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.replace('S&P 500', 'spx'))\n",
    "\n",
    "    # Transformar em String e Letras Minúsculas nas Mensagens\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: str(s).lower())\n",
    "\n",
    "    # Remover Pontuações\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "    # Remover Emojis     \n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: remove_emojis(s))\n",
    "\n",
    "    # Quebras de Linha desnecessárias\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.replace('\\n', ' '))\n",
    "\n",
    "    # Remover aspas duplas\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.replace('\\\"', ''))\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.replace('“', ''))\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.replace('”', ''))\n",
    "\n",
    "    # Remover valores\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: remove_valores(s))\n",
    "\n",
    "    # Espaços desnecessários\n",
    "    df_suno['title'] = df_suno['title'].map(lambda s: s.strip())\n",
    "    \n",
    "    # Extract titles\n",
    "    samples = df_suno['title'].values\n",
    "    \n",
    "    # Transformar os tópicos em números inteiros\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    df_suno['topic_number'] = label_encoder.fit_transform(df_suno['topic'])\n",
    "    class_names = label_encoder.classes_\n",
    "    \n",
    "    # Extract labels\n",
    "    labels = df_suno['topic_number'].values\n",
    "    \n",
    "    # Split Train/Test sets (80% Train / 20% Test --> Used for final metrics)\n",
    "    seed = 1337\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(samples)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(labels)\n",
    "\n",
    "    validation_split = 0.2\n",
    "    num_validation_samples = int(validation_split * len(samples))\n",
    "    train_samples = samples[:-num_validation_samples]\n",
    "    val_samples = samples[-num_validation_samples:]\n",
    "    train_labels = labels[:-num_validation_samples]\n",
    "    val_labels = labels[-num_validation_samples:]\n",
    "    \n",
    "    # Convert to number vector\n",
    "    from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "    vectorizer = TextVectorization(max_tokens=20000, \n",
    "                                   output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(32)\n",
    "    vectorizer.adapt(text_ds)\n",
    "    \n",
    "    print (text_ds)\n",
    "    \n",
    "    # Define the vocabulary\n",
    "    voc = [w.decode('utf-8') for w in vectorizer.get_vocabulary()]\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    \n",
    "    # Load pre-trained GloVe Embeddings\n",
    "    path_to_glove_file = '../../data/full_text_financial_news_vectors.txt'\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(path_to_glove_file, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "    \n",
    "    # Map words to vectors\n",
    "    num_tokens = len(voc) + 2\n",
    "    embedding_dim = 300\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    \n",
    "    # Apply transformations to train/valid. sets\n",
    "    x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "    x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "    y_train = np.array(train_labels)\n",
    "    y_val = np.array(val_labels)\n",
    "\n",
    "    x_test = x_val\n",
    "    y_test = y_val\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_layer = Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "    )\n",
    "\n",
    "    int_sequences_input = keras.Input(shape=(None,))\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM({{choice([32, 64])}},  return_sequences=True))(embedded_sequences)\n",
    "    x = tf.keras.layers.Dropout({{uniform(0, 1)}})(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM({{choice([32, 64])}}))(x)\n",
    "    x = tf.keras.layers.Dense({{choice([8, 16])}}, activation={{choice(['relu', 'tanh'])}})(x)\n",
    "    x = tf.keras.layers.Dropout({{uniform(0, 1)}})(x)\n",
    "    preds = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "    model = keras.Model(int_sequences_input, preds)\n",
    "    \n",
    "    # Compile and Fit\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        optimizer={{choice(['adam', 'rmsprop'])}}\n",
    "    )\n",
    "    \n",
    "    # 80% from X_train for training / 20% for Validation\n",
    "    result = model.fit(x_train, \n",
    "                       y_train,\n",
    "                       batch_size=32,\n",
    "                       verbose=2,\n",
    "                       validation_split=0.2)  \n",
    "    \n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_accuracy']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import string\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import metrics\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import Input, Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers import LSTM, Embedding\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import _locale\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'LSTM': hp.choice('LSTM', [32, 64]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'LSTM_1': hp.choice('LSTM_1', [32, 64]),\n",
      "        'Dense': hp.choice('Dense', [8, 16]),\n",
      "        'activation': hp.choice('activation', ['relu', 'tanh']),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'rmsprop']),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \n",
      "   3: # 1. Load the dataset\n",
      "   4: json_data_path = '../crawlers/suno/data/results-full-suno-2020.json'\n",
      "   5: \n",
      "   6: with open(json_data_path, 'r', encoding='utf8') as json_file:\n",
      "   7:     data = json.load(json_file)\n",
      "   8: \n",
      "   9: df_suno = pd.DataFrame(data)\n",
      "  10: \n",
      "  11: # Convert date to datetime format\n",
      "  12: df_suno['date'] = pd.to_datetime(df_suno['date'], format='%d/%m/%Y %H:%M')\n",
      "  13: \n",
      "  14: # Set date column as index\n",
      "  15: df_suno.set_index('date', inplace=True)\n",
      "  16: \n",
      "  17: # Order by date\n",
      "  18: df_suno.sort_index(inplace=True)\n",
      "  19: \n",
      "  20: # Convert text columns to string\n",
      "  21: df_suno['title'] = df_suno['title'].astype('string')\n",
      "  22: df_suno['full_text'] = df_suno['full_text'].astype('string')\n",
      "  23: \n",
      "  24: # Extract main columns\n",
      "  25: df_suno = df_suno[['title', 'full_text', 'topic']]\n",
      "  26: \n",
      "  27: # Select main topics\n",
      "  28: df_suno = df_suno[df_suno['topic'].isin(['Mercado', 'Economia', 'Internacional', 'Negócios'])]\n",
      "  29: \n",
      "  30: # 2. Preprocessing\n",
      "  31: def remove_emojis(sentence):\n",
      "  32: \n",
      "  33:     \"Remoção de Emojis nas mensagens de texto.\"\n",
      "  34: \n",
      "  35:     # Padrões dos Emojis\n",
      "  36:     emoji_pattern = re.compile(\"[\"\n",
      "  37:                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
      "  38:                 u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
      "  39:                 u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
      "  40:                 u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
      "  41:                 u\"\\U00002702-\\U000027B0\"\n",
      "  42:                 u\"\\U000024C2-\\U0001F251\"\n",
      "  43:                 u\"\\U0001f926-\\U0001f937\"\n",
      "  44:                 u'\\U00010000-\\U0010ffff'\n",
      "  45:                 u\"\\u200d\"\n",
      "  46:                 u\"\\u2640-\\u2642\"\n",
      "  47:                 u\"\\u2600-\\u2B55\"\n",
      "  48:                 u\"\\u23cf\"\n",
      "  49:                 u\"\\u23e9\"\n",
      "  50:                 u\"\\u231a\"\n",
      "  51:                 u\"\\u3030\"\n",
      "  52:                 u\"\\ufe0f\"\n",
      "  53:     \"]+\", flags=re.UNICODE)\n",
      "  54: \n",
      "  55:     return emoji_pattern.sub(r'', sentence)\n",
      "  56: \n",
      "  57: def remove_valores(sentence):\n",
      "  58:     new_sentece = ''\n",
      "  59: \n",
      "  60:     for token in sentence.split():\n",
      "  61:         if token.isdigit():\n",
      "  62:             token = '<NUM>'\n",
      "  63:         new_sentece += ' {}'.format(token)\n",
      "  64: \n",
      "  65:     return new_sentece\n",
      "  66: \n",
      "  67: # Substituir símbolos importantes\n",
      "  68: df_suno['title'] = df_suno['title'].map(lambda s: s.replace('-feira', ''))\n",
      "  69: df_suno['title'] = df_suno['title'].map(lambda s: s.replace('+', 'mais '))\n",
      "  70: df_suno['title'] = df_suno['title'].map(lambda s: s.replace('-', 'menos '))\n",
      "  71: df_suno['title'] = df_suno['title'].map(lambda s: s.replace('%', ' por cento'))\n",
      "  72: df_suno['title'] = df_suno['title'].map(lambda s: s.replace('R$', ''))\n",
      "  73: df_suno['title'] = df_suno['title'].map(lambda s: s.replace('U$', ''))\n",
      "  74: df_suno['title'] = df_suno['title'].map(lambda s: s.replace('US$', ''))\n",
      "  75: df_suno['title'] = df_suno['title'].map(lambda s: s.replace('S&P 500', 'spx'))\n",
      "  76: \n",
      "  77: # Transformar em String e Letras Minúsculas nas Mensagens\n",
      "  78: df_suno['title'] = df_suno['title'].map(lambda s: str(s).lower())\n",
      "  79: \n",
      "  80: # Remover Pontuações\n",
      "  81: df_suno['title'] = df_suno['title'].map(lambda s: s.translate(str.maketrans('', '', string.punctuation)))\n",
      "  82: \n",
      "  83: # Remover Emojis     \n",
      "  84: df_suno['title'] = df_suno['title'].map(lambda s: remove_emojis(s))\n",
      "  85: \n",
      "  86: # Quebras de Linha desnecessárias\n",
      "  87: df_suno['title'] = df_suno['title'].map(lambda s: s.replace('\\n', ' '))\n",
      "  88: \n",
      "  89: # Remover aspas duplas\n",
      "  90: df_suno['title'] = df_suno['title'].map(lambda s: s.replace('\\\"', ''))\n",
      "  91: df_suno['title'] = df_suno['title'].map(lambda s: s.replace('“', ''))\n",
      "  92: df_suno['title'] = df_suno['title'].map(lambda s: s.replace('”', ''))\n",
      "  93: \n",
      "  94: # Remover valores\n",
      "  95: df_suno['title'] = df_suno['title'].map(lambda s: remove_valores(s))\n",
      "  96: \n",
      "  97: # Espaços desnecessários\n",
      "  98: df_suno['title'] = df_suno['title'].map(lambda s: s.strip())\n",
      "  99: \n",
      " 100: # Extract titles\n",
      " 101: samples = df_suno['title'].values\n",
      " 102: \n",
      " 103: # Transformar os tópicos em números inteiros\n",
      " 104: label_encoder = preprocessing.LabelEncoder()\n",
      " 105: df_suno['topic_number'] = label_encoder.fit_transform(df_suno['topic'])\n",
      " 106: class_names = label_encoder.classes_\n",
      " 107: \n",
      " 108: # Extract labels\n",
      " 109: labels = df_suno['topic_number'].values\n",
      " 110: \n",
      " 111: # Split Train/Test sets (80% Train / 20% Test --> Used for final metrics)\n",
      " 112: seed = 1337\n",
      " 113: rng = np.random.RandomState(seed)\n",
      " 114: rng.shuffle(samples)\n",
      " 115: rng = np.random.RandomState(seed)\n",
      " 116: rng.shuffle(labels)\n",
      " 117: \n",
      " 118: validation_split = 0.2\n",
      " 119: num_validation_samples = int(validation_split * len(samples))\n",
      " 120: train_samples = samples[:-num_validation_samples]\n",
      " 121: val_samples = samples[-num_validation_samples:]\n",
      " 122: train_labels = labels[:-num_validation_samples]\n",
      " 123: val_labels = labels[-num_validation_samples:]\n",
      " 124: \n",
      " 125: # Convert to number vector\n",
      " 126: from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
      " 127: \n",
      " 128: MAX_SEQUENCE_LENGTH = 200\n",
      " 129: \n",
      " 130: vectorizer = TextVectorization(max_tokens=20000, \n",
      " 131:                                output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
      " 132: text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(32)\n",
      " 133: vectorizer.adapt(text_ds)\n",
      " 134: \n",
      " 135: print (text_ds)\n",
      " 136: \n",
      " 137: # Define the vocabulary\n",
      " 138: voc = [w.decode('utf-8') for w in vectorizer.get_vocabulary()]\n",
      " 139: word_index = dict(zip(voc, range(len(voc))))\n",
      " 140: \n",
      " 141: # Load pre-trained GloVe Embeddings\n",
      " 142: path_to_glove_file = '../../data/full_text_financial_news_vectors.txt'\n",
      " 143: \n",
      " 144: embeddings_index = {}\n",
      " 145: with open(path_to_glove_file, encoding='utf8') as f:\n",
      " 146:     for line in f:\n",
      " 147:         word, coefs = line.split(maxsplit=1)\n",
      " 148:         coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
      " 149:         embeddings_index[word] = coefs\n",
      " 150: \n",
      " 151: print(\"Found %s word vectors.\" % len(embeddings_index))\n",
      " 152: \n",
      " 153: # Map words to vectors\n",
      " 154: num_tokens = len(voc) + 2\n",
      " 155: embedding_dim = 300\n",
      " 156: hits = 0\n",
      " 157: misses = 0\n",
      " 158: \n",
      " 159: # Prepare embedding matrix\n",
      " 160: embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
      " 161: for word, i in word_index.items():\n",
      " 162:     embedding_vector = embeddings_index.get(word)\n",
      " 163:     if embedding_vector is not None:\n",
      " 164:         # Words not found in embedding index will be all-zeros.\n",
      " 165:         # This includes the representation for \"padding\" and \"OOV\"\n",
      " 166:         embedding_matrix[i] = embedding_vector\n",
      " 167:         hits += 1\n",
      " 168:     else:\n",
      " 169:         misses += 1\n",
      " 170: print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
      " 171: \n",
      " 172: # Apply transformations to train/valid. sets\n",
      " 173: x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
      " 174: x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
      " 175: \n",
      " 176: y_train = np.array(train_labels)\n",
      " 177: y_val = np.array(val_labels)\n",
      " 178: \n",
      " 179: x_test = x_val\n",
      " 180: y_test = y_val\n",
      " 181: \n",
      " 182: \n",
      " 183: \n",
      " 184: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     \"\"\"\n",
      "  13:     \n",
      "  14:     embedding_layer = Embedding(\n",
      "  15:         num_tokens,\n",
      "  16:         embedding_dim,\n",
      "  17:         embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
      "  18:         trainable=False,\n",
      "  19:     )\n",
      "  20: \n",
      "  21:     int_sequences_input = keras.Input(shape=(None,))\n",
      "  22:     embedded_sequences = embedding_layer(int_sequences_input)\n",
      "  23:     x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(space['LSTM'],  return_sequences=True))(embedded_sequences)\n",
      "  24:     x = tf.keras.layers.Dropout(space['Dropout'])(x)\n",
      "  25:     x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(space['LSTM_1']))(x)\n",
      "  26:     x = tf.keras.layers.Dense(space['Dense'], activation=space['activation'])(x)\n",
      "  27:     x = tf.keras.layers.Dropout(space['Dropout_1'])(x)\n",
      "  28:     preds = tf.keras.layers.Dense(len(class_names), activation='softmax')(x)\n",
      "  29: \n",
      "  30:     model = keras.Model(int_sequences_input, preds)\n",
      "  31:     \n",
      "  32:     # Compile and Fit\n",
      "  33:     model.compile(\n",
      "  34:         loss='sparse_categorical_crossentropy',\n",
      "  35:         metrics=['accuracy'],\n",
      "  36:         optimizer=space['optimizer']\n",
      "  37:     )\n",
      "  38:     \n",
      "  39:     # 80% from X_train for training / 20% for Validation\n",
      "  40:     result = model.fit(x_train, \n",
      "  41:                        y_train,\n",
      "  42:                        batch_size=32,\n",
      "  43:                        verbose=2,\n",
      "  44:                        validation_split=0.2)  \n",
      "  45:     \n",
      "  46:     #get the highest validation accuracy of the training epochs\n",
      "  47:     validation_acc = np.amax(result.history['val_accuracy']) \n",
      "  48:     print('Best validation acc of epoch:', validation_acc)\n",
      "  49:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      "  50: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (None,), types: tf.string>\n",
      "Found 149078 word vectors.\n",
      "Converted 9492 words (226 misses)\n",
      "338/338 - 11s - loss: 1.1164 - accuracy: 0.5710 - val_loss: 0.8394 - val_accuracy: 0.6964                              \n",
      "\n",
      "Best validation acc of epoch:                                                                                          \n",
      "0.696408748626709                                                                                                      \n",
      "338/338 - 12s - loss: 1.3203 - accuracy: 0.4823 - val_loss: 1.2382 - val_accuracy: 0.5165                              \n",
      "\n",
      "Best validation acc of epoch:                                                                                          \n",
      "0.5164753794670105                                                                                                     \n",
      " 20%|█████████▊                                       | 2/10 [00:33<02:12, 16.59s/trial, best loss: -0.696408748626709]"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=10,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name=\"5. Classificação de Textos com LSTM e Hyperopt\")\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Economia', 'Internacional', 'Mercado', 'Negócios']\n",
    "\n",
    "predicted = best_model.predict(X_test)\n",
    "\n",
    "Y_predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "print(metrics.classification_report(Y_test, Y_predicted, target_names=classes))\n",
    "print('Acurácia: {}'.format(metrics.accuracy_score(Y_test, Y_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.0",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
